# üõçÔ∏è Data Lakehouse E-commerce

Pipeline de Engenharia de Dados End-to-End simulando um ambiente de Lakehouse moderno, utilizando arquitetura Medalh√£o (Bronze, Silver, Gold).

## üèóÔ∏è Arquitetura e Tecnologias

Este projeto utiliza uma stack moderna baseada em containers:

* **Infraestrutura:** Docker & Docker Compose.
* **Orquestra√ß√£o:** Apache Airflow (DAGs agendadas).
* **Data Lake:** MinIO (S3 Compatible Object Storage).
* **Ingest√£o & Transforma√ß√£o:** Python, Pandas e Boto3.
* **Analytics (SQL on Files):** DuckDB (Processamento OLAP em arquivos Parquet).

## üîÑ Fluxo de Dados (ETL)

1.  **Camada Bronze (Raw):** Ingest√£o de dados brutos da [FakeStore API](https://fakestoreapi.com/) em formato **JSON**. Garante a fidelidade do dado original.
2.  **Camada Silver (Refined):** Limpeza, desnormaliza√ß√£o de campos aninhados (JSON Flattening) e convers√£o para formato colunar **Parquet** com compress√£o Snappy.
3.  **Camada Gold (Aggregated):** Execu√ß√£o de queries SQL anal√≠ticas via DuckDB para gerar KPIs de neg√≥cios (M√©dia de pre√ßos e avalia√ß√µes por categoria), exportados para **CSV**.

## üöÄ Como Executar

### Pr√©-requisitos
* Docker e Docker Compose instalados.

### Passo a Passo
1.  Clone o reposit√≥rio:
    ```bash
    git clone [https://github.com/SEU_USUARIO/data-lakehouse-ecommerce.git](https://github.com/SEU_USUARIO/data-lakehouse-ecommerce.git)
    ```
2.  Configure as vari√°veis de ambiente criando um .env:
    ```bash
    .env
    # Ajuste as credenciais se necess√°rio
    ```
3.  Suba o ambiente:
    ```bash
    docker compose up -d
    ```
4.  Acesse o Airflow (`http://localhost:8080`) e ative a DAG `pipeline_ecommerce_completo`.
5.  Acesse o MinIO (`http://localhost:9001`) para visualizar os buckets e arquivos gerados.

---
*Desenvolvido como projeto pr√°tico de Engenharia de Dados.*
